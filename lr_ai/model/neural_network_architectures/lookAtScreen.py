from lr_ai.features.tracker_toolbox import crop_face, get_detector, get_landmarks
from lr_ai.model.neuralNetwork import NeuralNetwork
import io
import cv2
from matplotlib import pyplot as plt
import numpy as np
import torch
from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd
from torch.optim import Adam
from torchvision import transforms
from torch import nn
import torch.nn.functional as F
from torchmetrics import Accuracy
from cnnvis import util
from torch.autograd import Variable

from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import BinaryClassifierOutputTarget, ClassifierOutputTarget, ClassifierOutputSoftmaxTarget
from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image
from PIL import Image



class CustomCrossEntropyLoss(nn.Module):
    def __init__(self):
        super(CustomCrossEntropyLoss, self).__init__()
        self.crit = nn.NLLLoss()

    def forward(self, outputs, targets):
        # Transformation étrange
        outputs = torch.log(outputs+1e-26)
        loss = self.crit(outputs, targets)
        return loss
    

class LookAtScreenClassification(NeuralNetwork):
    def __init__(self):
        NeuralNetwork.__init__(self)

        self.gradCamLayer = 0 #set the correct layer to be used by gradCam

    def confusion_mat(self, data, save_path="output/confusion_matrix.png"):
        """Computes and visualizes a confusion matrix to evaluate the performance of a classification model.

        Args:
            data (Pytorch DataLoader object): Dataset used for evaluating the model's performance

        Returns:
                y_pred (array of shape (data_number, target_number)): Predicted labels generated by the model for each sample in the input dataset.
                y_true (array of shape (data_number, target_number)): True labels corresponding to each sample in the input dataset.
        """

        y_pred = []
        y_true = []
        cpt=0
        # iterate over test data
        for img, target in data:
            cpt+=1
            predict = self.forward(img.to(self.device)) # Feed Network
            predict = (torch.max(torch.exp(predict), 1)[1]).data.cpu().numpy()

            y_pred.extend(predict) # Save Prediction

            target = target.data.cpu().numpy()

            y_true.extend(target) # Save Truth

        # constant for classes
        classes = ('lookAtScreen','other')

        # Build confusion matrix
        cf_matrix = confusion_matrix(y_true, y_pred)
        normalized_cf_matrix = cf_matrix / np.sum(cf_matrix)
        df_cm = pd.DataFrame(normalized_cf_matrix, 
                                       index=classes, 
                                       columns=classes)
        plt.figure(figsize = (12,7))
        sn.heatmap(df_cm, annot=True)
        plt.savefig(save_path)
        plt.show()

        return y_pred, y_true
    
    def accuracy(self, predict,targ, num_classes):
        """calculates the accuracy of a classification model using the provided predictions and true labels
        Args:
            predict (array of shape (data_number, target_number)): list or array containing the model's predictions.
            targ (array of shape (data_number, target_number)): list or array containing the true labels corresponding to the predictions.
            num_classes (int): number of classes in the classification problem.

        Returns:
            Acc (float): accuracy of the model.
        """
        acc = Accuracy(task="binary", num_classes=num_classes)
        return acc(torch.tensor(predict), torch.tensor(targ))


    def videoCap(self):
        """Captures real-time video from a webcam, applies transformations to the frames, and uses a classification model to predict the class of each frame.
        It displays the predicted class on the screen and terminates when the 'q' key is pressed.

        Args:
            model: model used for classification.
            transform: transformation function applied to the image frames.
        """
        # define transformation to apply to images
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
        ])
        cap = cv2.VideoCapture(0)
        detector = get_detector()
        
        while cap.isOpened():
            ret, frame = cap.read()

            landmarks = get_landmarks(frame,detector)
            if landmarks is not None:
                img = crop_face(frame, landmarks)
                img = transform(img)
            else:
                img = transform(frame)
            
            array = img.numpy()
            array_bgr = np.transpose(array, (1, 2, 0))
            scaled_array = ((array_bgr + 1) / 2) * 255
            showimg = scaled_array.astype(np.uint8)
            
            cv2.imshow("crop",showimg)

            img = img.unsqueeze(0).to(self.device)

            with torch.no_grad():
                outputs = self.forward(img)

            # Convertir les sorties en probabilités et trouver la classe prédite
            probs = torch.softmax(outputs, dim=1)
            pred_class = probs.argmax(dim=1)

            if pred_class.item() == 0:
                cv2.putText(frame, "Look at Screen", (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            else:
                cv2.putText(frame, "Don't", (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            cv2.imshow("Webcam", frame)

            if cv2.waitKey(1) == ord('q'):
                break

        cap.release()


    def videoCap_gradCam(self):
        """Captures real-time video from a webcam, applies transformations to the frames, and uses a classification model to predict the class of each frame.
        It displays the predicted class on the screen and terminates when the 'q' key is pressed.

        Args:
            model: model used for classification.
            transform: transformation function applied to the image frames.
        """
        # Définir les transformations à appliquer à chaque image

        cap = cv2.VideoCapture(0)
        detector = get_detector()

        target_layers = [self.architecture[self.gradCamLayer]]
        cam = GradCAM(model=self, target_layers=target_layers, use_cuda=False)

        while cap.isOpened():
            ret, img = cap.read()
            img = np.array(img)
            landmarks = get_landmarks(img,detector)
            if landmarks is not None:
                img = crop_face(img, landmarks)

            img = cv2.resize(img, (224, 224))
            img = np.float32(img) / 255
            input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

            targets = [ClassifierOutputSoftmaxTarget(1)]
            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)
            grayscale_cam = grayscale_cam[0, :]
            visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)
            
            with torch.no_grad():
                outputs = self.forward(input_tensor)

            probs = torch.softmax(outputs, dim=1)
            pred_class = probs.argmax(dim=1)

            if pred_class.item() == 0:
                cv2.putText(img, "Look at Screen", (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            else:
                cv2.putText(img, "Don't", (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)


            cv2.imshow("Webcam", img)
            cv2.imshow("Visualization",visualization)

            if cv2.waitKey(1) == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

    
    def gradCam(self, img_path, show=True, save_path='output/gradcam.png'): 
        """performs GradCAM visualization on a given image. It loads the image, 
        detects a face and crops it if needed. Then, the image is resized and normalized. 
        Predictions are obtained using a model and displayed. Using GradCAM, 
        the function generates a grayscale Class Activation Map (CAM) for the predicted class. 
        This map is overlayed on the original image and displayed.
        
         Args:
            img_path (string): Path to the image file
            show (bool): True to display the gradcam processing. Default to True.
            save_path (string): Path to save the processed image. Default to "output/gradcam.png"
        """

        


        target_layers = [self.architecture[self.gradCamLayer]]
        img = np.array(Image.open(img_path))
        detector = get_detector()

        #image crop
        landmarks = get_landmarks(img,detector)
        if landmarks is not None:
            img = crop_face(img, landmarks)
        
        img = cv2.resize(img, (224, 224))
        img = np.float32(img) / 255
        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(self.device, non_blocking=True)

        predict = self.forward(input_tensor)
        print("accuracy: ", predict)

        cam = GradCAM(model=self, target_layers=target_layers, use_cuda=False)

        targets = [ClassifierOutputSoftmaxTarget(1)]
        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)

        grayscale_cam = grayscale_cam[0, :]
        #input_array = np.array(np.transpose(input_tensor[0], (1, 2, 0)))

        visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)
        #print("visualization: ", visualization)
        plt.imshow(visualization)
        plt.savefig(save_path)
        if(show):
            plt.show()

    def saliencyMap(self, img_path, show=True, save_path='output/saliencymap.png'):
        """generates a saliency map for an image by computing gradients with respect to the image's pixels.
        It visualizes the original image alongside the saliency map, 
        which highlights the regions most influential in the model's prediction.
        
        Args:
            img_path (string): Path to the image file
            show (bool): True to display the gradcam processing. Default to True.
            save_path (string): Path to save the processed image. Default to "output/saliencymap.png"
        """
        
        image = np.array(Image.open(img_path))

        detector = get_detector()
        landmarks = get_landmarks(image,detector)
        if landmarks is not None:
            image = crop_face(image, landmarks)

        image = cv2.resize(image, (224, 224))
        image = np.float32(image) / 255
        input_tensor = preprocess_image(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])


        # Reshape the image (because the model use 
        # 4-dimensional tensor (batch_size, channel, width, height))
        image = input_tensor.reshape(1, 3, 224, 224)

        # Set the device for the image
        image = image.to(self.device)

        # Set the requires_grad_ to the image for retrieving gradients
        image.requires_grad_()

        # Retrieve output from the image
        output = self.forward(image)

        # Catch the output
        output_idx = output.argmax()
        output_max = output[0, output_idx]

        # Do backpropagation to get the derivative of the output based on the image
        output_max.backward()

        # Retireve the saliency map and also pick the maximum value from channels on each pixel.
        # In this case, we look at dim=1. Recall the shape (batch_size, channel, width, height)
        saliency, _ = torch.max(image.grad.data.abs(), dim=1) 
        saliency = saliency.reshape(224, 224)

        # Reshape the image
        image = image.reshape(-1, 224, 224)

        # Visualize the image and the saliency map
        fig, ax = plt.subplots(1, 2)
        ax[0].imshow(image.cpu().detach().numpy().transpose(1, 2, 0))
        ax[0].axis('off')
        ax[1].imshow(saliency.cpu(), cmap='hot')
        ax[1].axis('off')
        plt.tight_layout()
        fig.suptitle('The Image and Its Saliency Map')
        plt.savefig(save_path)
        if(show):
            plt.show()

    def Saliency_map(self, image_paths,method=util.GradType.GUIDED, show=False):
        """generates saliency maps for a list of images using guided backpropagation. 
        It plots the original images and their corresponding saliency maps side by side. 
        Finally, it computes the mean saliency map and returns it.
        
        Args:
            img_path (string): Path to the image file
            show (bool): True to display the gradcam processing. Default to True.
        """
        
        preprocess = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
        ])

        list_map = []
        map_sum = 0
        
        for i, image_path in enumerate(image_paths): 
            image=Image.open(image_path)
            image = np.array(image)
            method=util.GradType.GUIDED
            detector = get_detector()
            landmarks = get_landmarks(image,detector)
            if landmarks is not None:
                image = crop_face(image, landmarks)

            vis_param_dict, reset_state, remove_handles = util.augment_module(self)
            vis_param_dict['method'] = method
            img_tensor = preprocess(image)
            img_tensor.unsqueeze_(0)
            if self.device.type != 'cpu':
                img_tensor=img_tensor.cuda()
            input = Variable(img_tensor,requires_grad=True).to(self.device)
            
            if  input.grad is not None:
                input.grad.data.zero_()
            self.zero_grad()
            output = self.forward(input)
            ground_truth = np.argmax(output.detach().cpu().numpy())
            ind=torch.LongTensor(1)
            if(isinstance(ground_truth,np.int64)):
                ground_truth=ground_truth.item()
            ind[0]=ground_truth
            ind=Variable(ind)
            energy=output[0,ground_truth]
            energy.backward() 
            grad=input.grad
            if self.device == 'cpu':
                map1, output1 = np.abs(grad.data.numpy()[0]).max(axis=0), ground_truth
            else:
                map1, output1 = np.abs(grad.data.cpu().numpy()[0]).max(axis=0), ground_truth

            map1[map1 < 0.30 * np.max(map1)] =  0
            map1[map1 > 0.8 * np.max(map1)] =  0.8 * np.max(map1)
            map1 = (map1 - np.min(map1)) / (np.max(map1) - np.min(map1))
            list_map.append(map1)
            plt.figure(figsize=(15,15))
            plt.subplot(1,2,1)
            plt.subplots_adjust(top=0.85)
            plt.imshow(image)
            plt.axis('off') 
            plt.subplot(1,2,2)
            plt.imshow(map1,cmap='hot', interpolation='nearest')
            plt.axis('off')
            plt.title('Guided BP')
            plt.colorbar()
            plt.savefig(f'output/saliencymap{i+1}.png')
            if(show):
                plt.show()
            map_sum += map1

        map_sum /= len(list_map)

        plt.figure(figsize=(10,10))
        plt.imshow(map_sum,cmap='hot',interpolation='nearest')
        plt.savefig('output/mean_saliencymap.png')  
        if(show):
            plt.show()

        return map_sum

class lookAtScreenDistillation(LookAtScreenClassification):
    def __init__(self):
        super().__init__()

        self.gradCamLayer = 1 #set the correct layer to be used by gradCam
    

    def plotLoss(self, losses, losses_val):
        """plot loss of each sub-loss : Student-Teacher loss, Student, Ground-Truth loss, total loss and validation"""
        fig = plt.figure()
        plt.plot(losses[0],color='black')
        plt.plot(losses[1],color='blue')
        plt.plot(losses[2],color='green')
        plt.plot(losses_val,color='red')

        plt.legend(["final loss","SG","ST","validation"])

        plt.xlabel('epoch')
        plt.ylabel('loss')

        plt.grid(True)

        # displaying the title
        plt.title("Loss training")
        plt.show()
        fig.savefig("models/tmp/loss_history.png")

    
    
    
    
class TinyVGG(LookAtScreenClassification):
    """
    Documentation used : 
    https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb#scrollTo=82l6BusqlB9L
    Model architecture copying TinyVGG from: 
    https://poloclub.github.io/cnn-explainer/
    """
    def __init__(self, output_shape: int) -> None:
        super().__init__()
        
        #TinyVGG
        self.architecture.add_module('conv1',nn.Conv2d(3,10,5,1,1))
        self.architecture.add_module('relu1',nn.ReLU())
        self.architecture.add_module('maxPool1',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv2',nn.Conv2d(10,10,5,1,1))
        self.architecture.add_module('relu2',nn.ReLU())
        self.architecture.add_module('maxPool2',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv3',nn.Conv2d(10,10,5,1,1))
        self.architecture.add_module('relu3',nn.ReLU())
        self.architecture.add_module('maxPool3',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv4',nn.Conv2d(10,10,5,1,1))
        self.architecture.add_module('relu4',nn.ReLU())
        self.architecture.add_module('maxPool4',nn.MaxPool2d(2,2))

        self.architecture.add_module('flat5',nn.Flatten())
        self.architecture.add_module('lin5',nn.Linear(1440,output_shape))

        self.architecture.add_module('softmax10',nn.Softmax(dim=1))      

class VGG11(LookAtScreenClassification):
    """
    Documentation used : 
    https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb#scrollTo=82l6BusqlB9L
    Model architecture copying TinyVGG from: 
    https://poloclub.github.io/cnn-explainer/
    """
    def __init__(self, output_shape: int) -> None:
        super().__init__()

        #VGG11 custom
        #oublie pas ta rename les relu, avant t'avais que le premier qui était pris en compte
        self.architecture.add_module('conv1',nn.Conv2d(3,32,3,1,1))
        self.architecture.add_module('relu1',nn.ReLU())
        self.architecture.add_module('maxPool1',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv2',nn.Conv2d(32,64,3,1,1))
        self.architecture.add_module('relu2',nn.ReLU())
        self.architecture.add_module('maxPool2',nn.MaxPool2d(2,2))
        
        self.architecture.add_module('conv3a',nn.Conv2d(64,128,3,1,1))
        self.architecture.add_module('relu3a',nn.ReLU())
        self.architecture.add_module('conv3b',nn.Conv2d(128,128,3,1,1))
        self.architecture.add_module('relu3b',nn.ReLU())
        self.architecture.add_module('maxPool3',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv4a',nn.Conv2d(128,256,3,1,1))
        self.architecture.add_module('relu4a',nn.ReLU())
        self.architecture.add_module('conv4b',nn.Conv2d(256,256,3,1,1))
        self.architecture.add_module('relu4b',nn.ReLU())
        self.architecture.add_module('maxPool4',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv5a',nn.Conv2d(256,512,3,1,1))
        self.architecture.add_module('relu5a',nn.ReLU())
        self.architecture.add_module('conv5b',nn.Conv2d(512,512,3,1,1))
        self.architecture.add_module('relu5b',nn.ReLU())
        self.architecture.add_module('maxPool5',nn.MaxPool2d(2,2))

        self.architecture.add_module('flat6',nn.Flatten())

        self.architecture.add_module('linear7',nn.Linear(512*7*7,4096))
        self.architecture.add_module('relu7',nn.ReLU())
        self.architecture.add_module('dropout7',nn.Dropout2d())

        self.architecture.add_module('linear8',nn.Linear(4096,4096))
        self.architecture.add_module('relu8',nn.ReLU())
        self.architecture.add_module('dropout8',nn.Dropout2d())

        self.architecture.add_module('linear9',nn.Linear(4096,output_shape))

        self.architecture.add_module('softmax10',nn.Softmax(dim=1))      


class VGG11_Light(LookAtScreenClassification):
    """
    Documentation used : 
    https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb#scrollTo=82l6BusqlB9L
    Model architecture copying TinyVGG from: 
    https://poloclub.github.io/cnn-explainer/
    """
    def __init__(self, output_shape: int) -> None:
        super().__init__()

        #VGG11 custom
        architecture_conv = nn.Sequential()
        architecture_classif = nn.Sequential()
        architecture_activation = nn.Sequential()

        architecture_conv.add_module('conv1',nn.Conv2d(3,16,3,1,1))
        architecture_conv.add_module('maxPool1',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv2',nn.Conv2d(16,32,3,1,1))
        architecture_conv.add_module('maxPool2',nn.MaxPool2d(2,2))
        
        architecture_conv.add_module('conv3a',nn.Conv2d(32,64,3,1,1))
        architecture_conv.add_module('conv3b',nn.Conv2d(64,64,3,1,1))
        architecture_conv.add_module('maxPool3',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv4a',nn.Conv2d(64,128,3,1,1))
        architecture_conv.add_module('conv4b',nn.Conv2d(128,128,3,1,1))
        architecture_conv.add_module('maxPool4',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv5a',nn.Conv2d(128,128,3,1,1))
        architecture_conv.add_module('conv5b',nn.Conv2d(128,128,3,1,1))
        architecture_conv.add_module('maxPool5',nn.MaxPool2d(2,2))

        architecture_classif.add_module('flat6',nn.Flatten())

        architecture_classif.add_module('linear7',nn.Linear(128*7*7,2048))
        architecture_classif.add_module('dropout7',nn.Dropout())

        architecture_classif.add_module('linear8',nn.Linear(2048,2048))
        architecture_classif.add_module('dropout8',nn.Dropout())

        architecture_classif.add_module('linear9',nn.Linear(2048,output_shape))

        architecture_activation.add_module('softmax10',nn.Softmax(dim=1))

        self.architecture.add_module('conv_layers',architecture_conv.to(self.device))
        self.architecture.add_module('classif_layers',architecture_classif.to(self.device))
        self.architecture.add_module('activation_layer',architecture_activation.to(self.device))



class VGG11_Lighter(LookAtScreenClassification):
    """
    Documentation used : 
    https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb#scrollTo=82l6BusqlB9L
    Model architecture copying TinyVGG from: 
    https://poloclub.github.io/cnn-explainer/
    """
    def __init__(self, output_shape: int) -> None:
        super().__init__()

        #VGG11 custom

        self.architecture.add_module('conv1',nn.Conv2d(3,4,3,1,1))
        self.architecture.add_module('maxPool1',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv2',nn.Conv2d(4,8,3,1,1))
        self.architecture.add_module('maxPool2',nn.MaxPool2d(2,2))
        
        self.architecture.add_module('conv3a',nn.Conv2d(8,16,3,1,1))
        self.architecture.add_module('conv3b',nn.Conv2d(16,16,3,1,1))
        self.architecture.add_module('maxPool3',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv4a',nn.Conv2d(16,32,3,1,1))
        self.architecture.add_module('conv4b',nn.Conv2d(32,32,3,1,1))
        self.architecture.add_module('maxPool4',nn.MaxPool2d(2,2))

        self.architecture.add_module('conv5a',nn.Conv2d(32,32,3,1,1))
        self.architecture.add_module('conv5b',nn.Conv2d(32,32,3,1,1))
        self.architecture.add_module('maxPool5',nn.MaxPool2d(2,2))

        self.architecture.add_module('flat6',nn.Flatten())

        self.architecture.add_module('linear7',nn.Linear(32*7*7,512))
        self.architecture.add_module('dropout7',nn.Dropout())

        self.architecture.add_module('linear8',nn.Linear(512,512))
        self.architecture.add_module('dropout8',nn.Dropout())

        self.architecture.add_module('linear9',nn.Linear(512,output_shape))

        self.architecture.add_module('softmax10', nn.Softmax(dim=1))


class VGG11_Distillation(lookAtScreenDistillation):
    """
    Documentation used : 
    https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb#scrollTo=82l6BusqlB9L
    Model architecture copying TinyVGG from: 
    https://poloclub.github.io/cnn-explainer/
    """
    def __init__(self, output_shape: int, model: lookAtScreenDistillation = None) -> None:
        super().__init__()
        self.model=model

        self.teacher_coeff = 0.9
        self.student_coeff = 0.1

        architecture_conv = nn.Sequential()
        architecture_classif = nn.Sequential()
        architecture_activation = nn.Sequential()

        #VGG11 custom

        architecture_conv.add_module('conv1',nn.Conv2d(3,4,3,1,1))
        architecture_conv.add_module('maxPool1',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv2',nn.Conv2d(4,8,3,1,1))
        architecture_conv.add_module('maxPool2',nn.MaxPool2d(2,2))
        
        architecture_conv.add_module('conv3a',nn.Conv2d(8,16,3,1,1))
        architecture_conv.add_module('conv3b',nn.Conv2d(16,16,3,1,1))
        architecture_conv.add_module('maxPool3',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv4a',nn.Conv2d(16,32,3,1,1))
        architecture_conv.add_module('conv4b',nn.Conv2d(32,32,3,1,1))
        architecture_conv.add_module('maxPool4',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv5a',nn.Conv2d(32,32,3,1,1))
        architecture_conv.add_module('conv5b',nn.Conv2d(32,32,3,1,1))
        architecture_conv.add_module('maxPool5',nn.MaxPool2d(2,2))

        architecture_classif.add_module('flat6',nn.Flatten())

        architecture_classif.add_module('linear7',nn.Linear(32*7*7,512))
        architecture_classif.add_module('dropout7',nn.Dropout())

        architecture_classif.add_module('linear8',nn.Linear(512,512))
        architecture_classif.add_module('dropout8',nn.Dropout())

        architecture_classif.add_module('linear9',nn.Linear(512,output_shape))

        architecture_activation.add_module('softmax10', nn.Softmax(dim=1))

        self.architecture.add_module('conv_layers',architecture_conv.to(self.device))
        self.architecture.add_module('classif_layer',architecture_classif.to(self.device))
        self.architecture.add_module('activation_layer',architecture_activation.to(self.device))


    def loss_calculation(self, crit, inputs, target):
    
        outputs_student = self.architecture[1](self.architecture[0](inputs))
        outputs_teacher = self.model.architecture[1](self.architecture[0](inputs))
        
        student_softmax = F.softmax(outputs_student, dim=1)
        
        student_temp_softmax = F.softmax(outputs_student/10, dim=1)
        teacher_temp_softmax = F.softmax(outputs_teacher/10, dim=1)

        index_output_teacher = torch.argmax(teacher_temp_softmax, dim=1)
        #print(f"Min of outputs={outputs.min(1)} - Max of output = {outputs.max(1)}")
        lossTS = crit(student_temp_softmax,index_output_teacher)
        lossSG = crit(student_softmax, target)        
        loss = self.teacher_coeff * lossTS + self.student_coeff * lossSG

        return [loss, lossTS, lossSG]



class VGG11_KL_Distillation(lookAtScreenDistillation):
    """
    Documentation used : 
    https://medium.com/analytics-vidhya/knowledge-distillation-in-a-deep-neural-network-c9dd59aff89b
    """
    def __init__(self, output_shape: int, model: lookAtScreenDistillation = None) -> None:
        super().__init__()
        self.model=model
        self.all_loss_TS = []
        self.all_loss_SG = []

        self.teacher_coeff = 0.5
        self.student_coeff = 0.5

        architecture_conv = nn.Sequential()
        architecture_lastconv = nn.Sequential()
        architecture_lastmaxpool = nn.Sequential()
        architecture_classif = nn.Sequential()
        architecture_activation = nn.Sequential()

        #VGG11 custom

        architecture_conv.add_module('conv1',nn.Conv2d(3,4,3,1,1))
        architecture_conv.add_module('maxPool1',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv2',nn.Conv2d(4,8,3,1,1))
        architecture_conv.add_module('maxPool2',nn.MaxPool2d(2,2))
        
        architecture_conv.add_module('conv3a',nn.Conv2d(8,16,3,1,1))
        architecture_conv.add_module('conv3b',nn.Conv2d(16,16,3,1,1))
        architecture_conv.add_module('maxPool3',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv4a',nn.Conv2d(16,32,3,1,1))
        architecture_conv.add_module('conv4b',nn.Conv2d(32,32,3,1,1))
        architecture_conv.add_module('maxPool4',nn.MaxPool2d(2,2))

        architecture_conv.add_module('conv5a',nn.Conv2d(32,32,3,1,1))
        architecture_lastconv.add_module('conv5b',nn.Conv2d(32,32,3,1,1))
        architecture_lastmaxpool.add_module('maxPool5',nn.MaxPool2d(2,2))

        architecture_classif.add_module('flat6',nn.Flatten())

        architecture_classif.add_module('linear7',nn.Linear(32*7*7,512))
        architecture_classif.add_module('dropout7',nn.Dropout())

        architecture_classif.add_module('linear8',nn.Linear(512,512))
        architecture_classif.add_module('dropout8',nn.Dropout())

        architecture_classif.add_module('linear9',nn.Linear(512,output_shape))

        architecture_activation.add_module('softmax10', nn.Softmax(dim=1))

        self.architecture.add_module('conv_layers',architecture_conv.to(self.device))
        self.architecture.add_module('lastconv_layers',architecture_lastconv.to(self.device))
        self.architecture.add_module('lastpool_layers',architecture_lastmaxpool.to(self.device))
        self.architecture.add_module('classif_layer',architecture_classif.to(self.device))
        self.architecture.add_module('activation_layer',architecture_activation.to(self.device))

    def loss_calculation(self, crit, inputs, target):

        outputs_student = self.architecture[3](self.architecture[2](self.architecture[1](self.architecture[0](inputs))))
        outputs_teacher = self.model.architecture[1](self.model.architecture[0](inputs))
        
        student_softmax = F.softmax(outputs_student, dim=1)
        
        student_temp_softmax = F.softmax(outputs_student/10, dim=1)
        teacher_temp_softmax = F.softmax(outputs_teacher/10, dim=1)
        
        kl_loss = nn.KLDivLoss()
        lossTS = kl_loss(torch.log(student_temp_softmax),teacher_temp_softmax)
        lossSG = crit(student_softmax, target)

        self.all_loss_TS.append(lossTS)
        self.all_loss_SG.append(lossSG)
        loss = self.teacher_coeff * lossTS + self.student_coeff * lossSG

        return [loss, lossTS, lossSG]

      